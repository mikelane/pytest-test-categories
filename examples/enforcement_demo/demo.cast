{"version":3,"term":{"cols":80,"rows":24,"type":"xterm-256color"},"timestamp":1769659055,"command":"./demo-record.sh","env":{"SHELL":"/bin/zsh"}}
[0.296, "o", "\u001b[3J\u001b[H\u001b[2J"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n\u001b[1mpytest-test-categories: Enforcement Demo\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n\r\nThis plugin catches flaky test patterns and enforces hermetic testing.\r\n"]
[0.000, "o", "\r\n"]
[2.007, "o", "\r\n\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n"]
[0.000, "o", "\u001b[1mPart 1: Problem Tests (WARN Mode)\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n"]
[0.000, "o", "\r\nTests with violations - they PASS but violations are logged:\r\n\r\n"]
[0.000, "o", "\u001b[1;33m$ pytest --test-categories-enforcement=warn tests/test_problem_tests.py -v --tb=no\u001b[0m\r\n"]
[2.677, "o", "\u001b]9;4;3;\u001b\\"]
[0.001, "o", "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n"]
[0.000, "o", "platform darwin -- Python 3.13.5, pytest-9.0.0, pluggy-1.6.0 -- /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/.venv/bin/python"]
[0.000, "o", "\r\n"]
[0.006, "o", "cachedir: .pytest_cache\r\nbenchmark: 5.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/examples/enforcement_demo\r\nconfigfile: pyproject.toml\r\nplugins: benchmark-5.2.3, mock-3.15.1, test-categories-1.2.0, xdist-3.8.0, bdd-8.1.0, cov-7.0.0\r\n"]
[0.000, "o", "\u001b[1mcollecting ... \u001b[0m"]
[0.011, "o", "\u001b[1m\rcollected 11 items                                                             \u001b[0m\r\n"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeSleepViolations::test_cache_expiration_with_sleep [SMALL] "]
[0.209, "o", "\u001b]9;4;1;0\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [  9%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeSleepViolations::test_polling_with_sleep [SMALL] "]
[0.057, "o", "\u001b]9;4;1;9\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 18%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeNetworkViolations::test_fetch_from_real_api [SMALL] "]
[0.441, "o", "\u001b]9;4;1;18\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m\u001b[33m [ 27%]\u001b[0m"]
[0.001, "o", "\r\ntests/test_problem_tests.py::DescribeNetworkViolations::test_check_external_service_health [SMALL] "]
[0.038, "o", "\u001b]9;4;1;27\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 36%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeFilesystemViolations::test_read_system_file [SMALL] "]
[0.002, "o", "\u001b]9;4;1;36\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 45%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeFilesystemViolations::test_write_to_current_directory [SMALL] "]
[0.001, "o", "\u001b]9;4;1;45\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 54%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeSubprocessViolations::test_run_shell_command [SMALL] "]
[0.013, "o", "\u001b]9;4;1;54\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 63%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeSubprocessViolations::test_shell_runner_real_execution [SMALL] "]
[0.011, "o", "\u001b]9;4;1;63\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 72%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_problem_tests.py::DescribeThreadingViolations::test_background_worker_with_timing [SMALL] "]
[0.206, "o", "\u001b]9;4;1;72\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 81%]\u001b[0m"]
[0.001, "o", "\r\ntests/test_problem_tests.py::DescribeThreadingViolations::test_concurrent_operations [SMALL] "]
[0.001, "o", "\u001b]9;4;1;81\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [ 90%]\u001b[0m"]
[0.001, "o", "\r\ntests/test_problem_tests.py::DescribeCombinedViolations::test_fetch_save_and_wait [SMALL] "]
[0.054, "o", "\u001b]9;4;1;90\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[33m [100%]\u001b[0m"]
[0.000, "o", "\u001b]9;4;0;\u001b\\"]
[0.001, "o", "\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[33m=============================== warnings summary ===============================\u001b[0m\r\n"]
[0.000, "o", "tests/test_problem_tests.py::DescribeSleepViolations::test_polling_with_sleep [SMALL]\r\n"]
[0.000, "o", "  /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/examples/enforcement_demo/tests/test_problem_tests.py:84: UserWarning: Small test 'tests/test_problem_tests.py::DescribeSleepViolations::test_polling_with_sleep [SMALL]' uses threading.Thread. Small tests should be single-threaded for determinism. Consider using @pytest.mark.medium if concurrency testing is required.\r\n    threading.Timer(0.05, make_ready).start()\r\n\r\ntests/test_problem_tests.py::DescribeThreadingViolations::test_background_worker_with_timing [SMALL]\r\n  /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/examples/enforcement_demo/tests/test_problem_tests.py:289: UserWarning: Small test 'tests/test_problem_tests.py::DescribeThreadingViolations::test_background_worker_with_timing [SMALL]' uses threading.Thread. Small tests should be single-threaded for determinism. Consider using @pytest.mark.medium if concurrency testing is required.\r\n    worker.start()\r\n\r\ntests/test_problem_tests.py::"]
[0.000, "o", "DescribeThreadingViolations::test_concurrent_operations [SMALL]"]
[0.000, "o", "\r\n  /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/.venv/lib/python3.13/site-packages/_pytest/python.py:166: UserWarning: Small test 'tests/test_problem_tests.py::DescribeThreadingViolations::test_concurrent_operations [SMALL]' uses threading.Thread. Small tests should be single-threaded for determinism. Consider using @pytest.mark.medium if concurrency testing is required.\r\n    result = testfunction(**testargs)\r\n\r\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n"]
[0.000, "o", "======================= Test Suite Distribution Summary ========================\r\n    Test Size Distribution:\r\n"]
[0.000, "o", "      Small     11 tests (100.00%)\r\n      Medium     0 tests (0.00%)\r\n"]
[0.000, "o", "      Large      0 tests (0.00%)"]
[0.000, "o", "\r\n"]
[0.000, "o", "      XLarge     0 tests (0.00%)\r\n\r\n    Status: Great job! Your test distribution is on track.\r\n"]
[0.000, "o", "================================================================================"]
[0.000, "o", "\r\n"]
[0.000, "o", "======================== Hermeticity Violation Summary =========================\r\nViolations detected (enforcement: warn):"]
[0.000, "o", "\r\n"]
[0.000, "o", "  Network:     3 tests (tests/test_problem_tests.py::DescribeNetworkViolations::test_fetch_from_real_api [SMALL], tests/test_problem_tests.py::DescribeNetworkViolations::test_check_external_service_health [SMALL], tests/test_problem_tests.py::DescribeCombinedViolations::test_fetch_save_and_wait [SMALL])\r\n  Filesystem:  14 tests (tests/test_problem_tests.py::DescribeFilesystemViolations::test_read_system_file [SMALL], tests/test_problem_tests.py::DescribeFilesystemViolations::test_read_system_file [SMALL], tests/test_problem_tests.py::DescribeFilesystemViolations::test_write_to_current_directory [SMALL], ...+11 more)\r\n"]
[0.000, "o", "  Process:     4 tests (tests/test_problem_tests.py::DescribeSubprocessViolations::test_run_shell_command [SMALL], tests/test_problem_tests.py::DescribeSubprocessViolations::test_run_shell_command [SMALL], tests/test_problem_tests.py::DescribeSubprocessViolations::test_shell_runner_real_execution [SMALL], ...+1 more)\r\n"]
[0.000, "o", "  Database:    0 tests\r\n  Sleep:       4 tests (tests/test_problem_tests.py::DescribeSleepViolations::test_cache_expiration_with_sleep [SMALL], tests/test_problem_tests.py::DescribeSleepViolations::test_polling_with_sleep [SMALL], tests/test_problem_tests.py::DescribeThreadingViolations::test_background_worker_with_timing [SMALL], ...+1 more)\r\n"]
[0.000, "o", "\r\nTotal: 25 violations in 10 tests"]
[0.000, "o", "\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "To fix: Mock external dependencies or change test category to @pytest.mark.medium\r\nDocs: https://pytest-test-categories.readthedocs.io/resource-isolation/\r\n================================================================================"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[33m======================== \u001b[32m11 passed\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 1.05s\u001b[0m\u001b[33m ========================\u001b[0m\r\n"]
[3.108, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n\u001b[1mPart 2: STRICT Mode - Violations FAIL\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n\r\nSame tests, but now violations cause test failures:\r\n\r\n\u001b[1;33m$ pytest --test-categories-enforcement=strict tests/test_problem_tests.py --tb=line -q\u001b[0m\r\n"]
[2.488, "o", "\u001b]9;4;3;\u001b\\"]
[0.017, "o", "\u001b]9;4;2;0\u001b\\"]
[0.000, "o", "\u001b[31mF\u001b[0m"]
[0.004, "o", "\u001b]9;4;2;9\u001b\\"]
[0.000, "o", "\u001b[31mF\u001b[0m"]
[0.017, "o", "\u001b]9;4;2;18\u001b\\"]
[0.000, "o", "\u001b[33ms\u001b[0m"]
[0.002, "o", "\u001b]9;4;2;27\u001b\\\u001b[31mF\u001b[0m"]
[0.001, "o", "\u001b]9;4;2;36\u001b\\\u001b[31mF\u001b[0m"]
[0.002, "o", "\u001b]9;4;2;45\u001b\\\u001b[31mF\u001b[0m"]
[0.001, "o", "\u001b]9;4;2;54\u001b\\\u001b[31mF\u001b[0m"]
[0.001, "o", "\u001b]9;4;2;63\u001b\\\u001b[31mF\u001b[0m"]
[0.002, "o", "\u001b]9;4;2;72\u001b\\"]
[0.000, "o", "\u001b[31mF\u001b[0m"]
[0.001, "o", "\u001b]9;4;2;81\u001b\\"]
[0.000, "o", "\u001b[32m.\u001b[0m"]
[0.002, "o", "\u001b]9;4;2;90\u001b\\\u001b[31mF\u001b[0m"]
[0.000, "o", "\u001b[31m                                                              [100%]\u001b[0m"]
[0.000, "o", "\u001b]9;4;0;\u001b\\"]
[0.000, "o", "\r\n"]
[0.000, "o", "=================================== FAILURES ===================================\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.SleepViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    [TC005] Sleep Call Violation\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeSleepViolations::test_cache_expiration_with_sleep [SMALL]\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      Called time.sleep(0.2) - attempted to sleep for 0.2 seconds\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n\u001b[1m\u001b[31m      Small tests should not call sleep functions. Using sleep in tests indicates waiting for async operations that should use proper synchronization, flaky timing assumptions, or polling patterns that should use condition-based waiting instead.\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m\r\n\u001b[1m\u001b[31m      • Use proper synchronization instead of sleep (e.g., threading.Event)\u001b[0m\r\n\u001b[1m\u001b[31m      • Use condition-based waiting with polling and timeout\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Mock time.sleep using pytest-mock (mocker.patch)\u001b[0m\r\n\u001b[1m\u001b[31m      • Use a FakeTimer or controllable time abstraction\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if timing is required)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/sleep-blocking.html\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/sleep.py:185: pytest_test_categories.exceptions.SleepViolationError:\r\n"]
[0.001, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.SleepViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    [TC005] Sleep Call Violation\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeSleepViolations::test_polling_with_sleep [SMALL]\u001b[0m\r\n\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      Called time.sleep(0.05) - attempted to sleep for 0.05 seconds\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n\u001b[1m\u001b[31m      Small tests should not call sleep functions. Using sleep in tests indicates waiting for async operations that should use proper synchronization, flaky timing assumptions, or polling patterns that should use condition-based waiting instead.\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m\r\n\u001b[1m\u001b[31m      • Use proper synchronization instead of sleep (e.g., threading.Event)\u001b[0m\r\n\u001b[1m\u001b[31m      • Use condition-based waiting with polling and timeout\u001b[0m\r\n\u001b[1m\u001b[31m      • Mock time.sleep using pytest-mock (mocker.patch)\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Use a FakeTimer or controllable time abstraction\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if timing is required)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/sleep-blocking.html\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/sleep.py:185: pytest_test_categories.exceptions.SleepViolationError:\r\n\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.NetworkAccessViolationError: \u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    [TC001] Network Access Violation\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeNetworkViolations::test_check_external_service_health [SMALL]\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n\u001b[1m\u001b[31m      Attempted network connection to google.com:443\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      Small tests must be hermetic and cannot access the network. Network calls introduce non-determinism, external dependencies, and can cause flaky tests due to network latency or failures.\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m\r\n\u001b[1m\u001b[31m      • Mock the network call using responses, httpretty, or respx\u001b[0m\r\n\u001b[1m\u001b[31m      • Use dependency injection to provide a fake HTTP client\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if network access is required)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/network-isolation.html\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/network.py:167: pytest_test_categories.exceptions.NetworkAccessViolationError:"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.FilesystemAccessViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    [TC002] Filesystem Access Violation\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeFilesystemViolations::test_read_system_file [SMALL]\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    What happened:\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m      Attempted read on filesystem path: /etc/passwd\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n\u001b[1m\u001b[31m      Small tests should not access the filesystem directly. Filesystem access introduces I/O overhead, potential race conditions, and dependencies on the test environment state.\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Use pyfakefs for comprehensive filesystem mocking (pip install pyfakefs)\u001b[0m\r\n\u001b[1m\u001b[31m      • Use io.StringIO or io.BytesIO for in-memory file-like objects\u001b[0m\r\n\u001b[1m\u001b[31m      • Mock file operations using pytest-mock (mocker.patch(\"builtins.open\", ...))\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Embed test data as Python constants or use importlib.resources\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if filesystem access is required)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/filesystem-isolation.html\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/filesystem.py:233: pytest_test_categories.exceptions.FilesystemAccessViolationError:"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.FilesystemAccessViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    [TC002] Filesystem Access Violation\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeFilesystemViolations::test_write_to_current_directory [SMALL]\u001b[0m\r\n\u001b[1m\u001b[31m    Category: SMALL\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n\u001b[1m\u001b[31m      Attempted write on filesystem path: test_output_file.txt\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n\u001b[1m\u001b[31m      Small tests should not access the filesystem directly. Filesystem access introduces I/O overhead, potential race conditions, and dependencies on the test environment state.\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m\r\n\u001b[1m\u001b[31m      • Use pyfakefs for comprehensive filesystem mocking (pip install pyfakefs)\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Use io.StringIO or io.BytesIO for in-memory file-like objects\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Mock file operations using pytest-mock (mocker.patch(\"builtins.open\", ...))\u001b[0m\r\n\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if filesystem access is required)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/filesystem-isolation.html\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/filesystem.py:233: pytest_test_categories.exceptions.FilesystemAccessViolationError:"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.SubprocessViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    [TC003] Subprocess Spawn Violation\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeSubprocessViolations::test_run_shell_command [SMALL]\u001b[0m\r\n\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n\u001b[1m\u001b[31m      Attempted subprocess.run: ls -la\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n\u001b[1m\u001b[31m      Small tests should run in a single process without spawning subprocesses. Subprocess spawning introduces non-determinism from external process behavior, I/O overhead from process creation, and timing variability that causes flaky tests.\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Mock subprocess.run using pytest-mock (mocker.patch)\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m      • Use dependency injection to provide a fake command executor\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Test the logic that prepares subprocess arguments, not the spawn itself\u001b[0m\r\n\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (pytester spawns subprocesses)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/process-isolation.html\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/process.py:237: pytest_test_categories.exceptions.SubprocessViolationError:\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.SubprocessViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    [TC003] Subprocess Spawn Violation\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeSubprocessViolations::test_shell_runner_real_execution [SMALL]\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n\u001b[1m\u001b[31m      Attempted subprocess.run: git --version\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n\u001b[1m\u001b[31m      Small tests should run in a single process without spawning subprocesses. Subprocess spawning introduces non-determinism from external process behavior, I/O overhead from process creation, and timing variability that causes flaky tests.\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Mock subprocess.run using pytest-mock (mocker.patch)\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m      • Use dependency injection to provide a fake command executor\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Test the logic that prepares subprocess arguments, not the spawn itself\u001b[0m\r\n\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (pytester spawns subprocesses)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/process-isolation.html\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/process.py:237: pytest_test_categories.exceptions.SubprocessViolationError:"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.SleepViolationError: \u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    [TC005] Sleep Call Violation\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeThreadingViolations::test_background_worker_with_timing [SMALL]\u001b[0m\r\n\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    What happened:\u001b[0m\r\n\u001b[1m\u001b[31m      Called time.sleep(0.2) - attempted to sleep for 0.2 seconds\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Why it matters:\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      Small tests should not call sleep functions. Using sleep in tests indicates waiting for async operations that should use proper synchronization, flaky timing assumptions, or polling patterns that should use condition-based waiting instead.\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Use proper synchronization instead of sleep (e.g., threading.Event)\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m      • Use condition-based waiting with polling and timeout\u001b[0m\r\n\u001b[1m\u001b[31m      • Mock time.sleep using pytest-mock (mocker.patch)\u001b[0m\r\n\u001b[1m\u001b[31m      • Use a FakeTimer or controllable time abstraction\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if timing is required)\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/sleep-blocking.html\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/sleep.py:185: pytest_test_categories.exceptions.SleepViolationError:\r\n"]
[0.000, "o", "\u001b[1m\u001b[31mE   pytest_test_categories.exceptions.NetworkAccessViolationError: \u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n\u001b[1m\u001b[31m    [TC001] Network Access Violation\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Test: tests/test_problem_tests.py::DescribeCombinedViolations::test_fetch_save_and_wait [SMALL]\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    Category: SMALL\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    What happened:\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m      Attempted network connection to localhost:9999\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    Why it matters:\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      Small tests must be hermetic and cannot access the network. Network calls introduce non-determinism, external dependencies, and can cause flaky tests due to network latency or failures.\u001b[0m\r\n\u001b[1m\u001b[31m    \u001b[0m\r\n\u001b[1m\u001b[31m    To fix this (choose one):\u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m      • Mock the network call using responses, httpretty, or respx\u001b[0m"]
[0.000, "o", "\r\n\u001b[1m\u001b[31m      • Use dependency injection to provide a fake HTTP client\u001b[0m\r\n\u001b[1m\u001b[31m      • Change test category to @pytest.mark.medium (if network access is required)\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    \u001b[0m"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[31m    See: https://pytest-test-categories.readthedocs.io/en/latest/errors/network-isolation.html\u001b[0m\r\n\u001b[1m\u001b[31m    ======================================================================\u001b[0m\r\n"]
[0.000, "o", "/Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/src/pytest_test_categories/adapters/network.py:167: pytest_test_categories.exceptions.NetworkAccessViolationError:\r\n"]
[0.001, "o", "======================= Test Suite Distribution Summary ========================\r\n    Test Size Distribution:\r\n"]
[0.000, "o", "      Small     11 tests (100.00%)"]
[0.000, "o", "\r\n"]
[0.000, "o", "      Medium     0 tests (0.00%)\r\n      Large      0 tests (0.00%)\r\n      XLarge     0 tests (0.00%)\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "    Status: Great job! Your test distribution is on track.\r\n================================================================================\r\n"]
[0.000, "o", "======================== Hermeticity Violation Summary =========================\r\nViolations detected (enforcement: strict):\r\n"]
[0.000, "o", "  Network:     3 tests"]
[0.000, "o", "\r\n"]
[0.000, "o", "  Filesystem:  2 tests\r\n  Process:     2 tests\r\n  Sleep:       3 tests\r\n\r\n"]
[0.000, "o", "Total: 10 violations in 10 tests\r\n10 tests failed due to violations\r\n\r\n"]
[0.000, "o", "To fix: Mock external dependencies or change test category to @pytest.mark.medium\r\nDocs: https://pytest-test-categories.readthedocs.io/resource-isolation/\r\n"]
[0.000, "o", "================================================================================"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\r\n\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeSleepViolations::test_cache_expiration_with_sleep [SMALL]\u001b[0m - pytest_test_categories.exceptions.SleepViolationError: \r\n\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeSleepViolations::test_polling_with_sleep [SMALL]\u001b[0m - pytest_test_categories.exceptions.SleepViolationError: \r\n"]
[0.000, "o", "\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeNetworkViolations::test_check_external_service_health [SMALL]\u001b[0m - pytest_test_categories.exceptions.NetworkAccessViolationError: \r\n\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeFilesystemViolations::test_read_system_file [SMALL]\u001b[0m - pytest_test_categories.exceptions.FilesystemAccessViolationError: \r\n\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeFilesystemViolations::test_write_to_current_directory [SMALL]\u001b[0m - pytest_test_categories.exceptions.FilesystemAccessViolationError: \r\n\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeSubprocessViolations::test_run_shell_command [SMALL]\u001b[0m - pytest_test_categories.exceptions.SubprocessViolationError: \r\n"]
[0.000, "o", "\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeSubprocessViolations::test_shell_runner_real_execution [SMALL]\u001b[0m - pytest_test_categories.exceptions.SubprocessViolationError: \r\n"]
[0.000, "o", "\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeThreadingViolations::test_background_worker_with_timing [SMALL]\u001b[0m - pytest_test_categories.exceptions.SleepViolationError: "]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[31mFAILED\u001b[0m tests/test_problem_tests.py::\u001b[1mDescribeCombinedViolations::test_fetch_save_and_wait [SMALL]\u001b[0m - pytest_test_categories.exceptions.NetworkAccessViolationError: "]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[31m\u001b[31m\u001b[1m9 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[0m\r\n"]
[3.110, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n"]
[0.000, "o", "\u001b[1mPart 3: Hermetic Tests PASS in Strict Mode\u001b[0m\r\n\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "Properly written tests use mocking and dependency injection:\r\n\r\n"]
[0.000, "o", "\u001b[1;33m$ pytest --test-categories-enforcement=strict tests/test_solution_tests.py -v\u001b[0m\r\n"]
[2.714, "o", "\u001b]9;4;3;\u001b\\"]
[0.002, "o", "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n"]
[0.000, "o", "platform darwin -- Python 3.13.5, pytest-9.0.0, pluggy-1.6.0 -- /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/.venv/bin/python"]
[0.000, "o", "\r\n"]
[0.005, "o", "cachedir: .pytest_cache\r\nbenchmark: 5.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: /Users/mikelane/dev/pytest-test-categories/.worktrees/issue-221-enforcement-demo/examples/enforcement_demo\r\n"]
[0.000, "o", "configfile: pyproject.toml\r\nplugins: benchmark-5.2.3, mock-3.15.1, test-categories-1.2.0, xdist-3.8.0, bdd-8.1.0, cov-7.0.0"]
[0.000, "o", "\r\n"]
[0.000, "o", "\u001b[1mcollecting ... \u001b[0m"]
[0.025, "o", "\u001b[1m\rcollected 26 items                                                             \u001b[0m\r\n"]
[0.001, "o", "\r\ntests/test_solution_tests.py::DescribeCacheWithControllableTime::test_cache_stores_and_retrieves_value [SMALL] "]
[0.005, "o", "\u001b]9;4;1;0\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [  3%]\u001b[0m"]
[0.001, "o", "\r\ntests/test_solution_tests.py::DescribeCacheWithControllableTime::test_cache_expires_after_ttl [SMALL] "]
[0.001, "o", "\u001b]9;4;1;3\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [  7%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeCacheWithControllableTime::test_cache_entry_valid_before_ttl [SMALL] "]
[0.000, "o", "\u001b]9;4;1;7\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 11%]\u001b[0m"]
[0.001, "o", "\r\ntests/test_solution_tests.py::DescribeCacheWithControllableTime::test_cache_custom_ttl [SMALL] "]
[0.000, "o", "\u001b]9;4;1;11\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 15%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeApiClientWithMocking::test_get_user_with_injected_http [SMALL] "]
[0.001, "o", "\u001b]9;4;1;15\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 19%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeApiClientWithMocking::test_list_users_with_injected_http [SMALL] "]
[0.001, "o", "\u001b]9;4;1;19\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 23%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeApiClientWithMocking::test_health_check_success [SMALL] "]
[0.001, "o", "\u001b]9;4;1;23\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 26%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeApiClientWithMocking::test_health_check_failure [SMALL] "]
[0.000, "o", "\u001b]9;4;1;26\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.001, "o", "\u001b[32m [ 30%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeApiClientWithMocking::test_health_check_network_error [SMALL] "]
[0.000, "o", "\u001b]9;4;1;30\u001b\\\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 34%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeApiClientWithMocking::test_user_model_creation [SMALL] "]
[0.001, "o", "\u001b]9;4;1;34\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 38%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeConfigWithInMemory::test_load_config_from_stream [SMALL] "]
[0.001, "o", "\u001b]9;4;1;38\u001b\\\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 42%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeConfigWithInMemory::test_config_defaults [SMALL] "]
[0.001, "o", "\u001b]9;4;1;42\u001b\\\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 46%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeConfigWithInMemory::test_config_partial_override [SMALL] "]
[0.001, "o", "\u001b]9;4;1;46\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 50%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeConfigWithInMemory::test_app_config_model [SMALL] "]
[0.001, "o", "\u001b]9;4;1;50\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 53%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeShellRunnerWithFakeExecutor::test_git_version_with_fake_executor [SMALL] "]
[0.001, "o", "\u001b]9;4;1;53\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 57%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeShellRunnerWithFakeExecutor::test_git_not_installed [SMALL] "]
[0.001, "o", "\u001b]9;4;1;57\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 61%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeShellRunnerWithFakeExecutor::test_list_directory_with_fake_executor [SMALL] "]
[0.001, "o", "\u001b]9;4;1;61\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 65%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeShellRunnerWithFakeExecutor::test_disk_usage_with_fake_executor [SMALL] "]
[0.000, "o", "\u001b]9;4;1;65\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.001, "o", "\u001b[32m [ 69%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeShellRunnerWithFakeExecutor::test_command_failure [SMALL] "]
[0.000, "o", "\u001b]9;4;1;69\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 73%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeWorkerLogicSynchronously::test_process_job_success [SMALL] "]
[0.001, "o", "\u001b]9;4;1;73\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 76%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeWorkerLogicSynchronously::test_process_job_with_error [SMALL] "]
[0.001, "o", "\u001b]9;4;1;76\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 80%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeWorkerLogicSynchronously::test_job_model_initial_state [SMALL] "]
[0.001, "o", "\u001b]9;4;1;80\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 84%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeWorkerLogicSynchronously::test_multiple_jobs_processed_independently [SMALL] "]
[0.000, "o", "\u001b]9;4;1;84\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.001, "o", "\u001b[32m [ 88%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeMediumIntegrationTests::test_background_worker_integration [MEDIUM] "]
[0.001, "o", "\u001b]9;4;1;88\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 92%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeMediumIntegrationTests::test_shell_runner_with_real_command [MEDIUM] "]
[0.008, "o", "\u001b]9;4;1;92\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [ 96%]\u001b[0m"]
[0.000, "o", "\r\ntests/test_solution_tests.py::DescribeMediumIntegrationTests::test_config_with_temp_file [MEDIUM] "]
[0.005, "o", "\u001b]9;4;1;96\u001b\\"]
[0.000, "o", "\u001b[32mPASSED\u001b[0m"]
[0.000, "o", "\u001b[32m [100%]\u001b[0m"]
[0.001, "o", "\u001b]9;4;0;\u001b\\"]
[0.000, "o", "\r\n"]
[0.000, "o", "======================= Test Suite Distribution Summary ========================\r\n\r\n    Test Size Distribution:"]
[0.000, "o", "\r\n"]
[0.000, "o", "      Small     23 tests (88.46%)\r\n      Medium     3 tests (11.54%)\r\n"]
[0.000, "o", "      Large      0 tests (0.00%)"]
[0.000, "o", "\r\n      XLarge     0 tests (0.00%)\r\n\r\n    Status: Great job! Your test distribution is on track."]
[0.000, "o", "\r\n"]
[0.000, "o", "================================================================================\r\n"]
[0.000, "o", "\u001b[32m============================== \u001b[32m\u001b[1m26 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m ==============================\u001b[0m\r\n"]
[2.113, "o", "\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n"]
[0.000, "o", "\u001b[1mSummary\u001b[0m\r\n"]
[0.000, "o", "\u001b[1m\u001b[0;34m══════════════════════════════════════════════════════════════\u001b[0m\r\n\r\n"]
[0.000, "o", "\u001b[0;32mHermetic tests = Fast, Reliable, Deterministic\u001b[0m\r\n\r\nAdoption path:\r\n"]
[0.000, "o", "  1. enforcement=off   - Explore your codebase\r\n  2. enforcement=warn  - See violations, fix incrementally\r\n"]
[0.000, "o", "  3. enforcement=strict - Prevent regressions\r\n\r\n"]
[0.000, "o", "\u001b[0;36mpip install pytest-test-categories\u001b[0m\r\n\r\n"]
[3.005, "x", "0"]
